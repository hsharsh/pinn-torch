{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import grad\n",
    "\n",
    "# device = torch.device('cuda')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "sys.path.append('utils_discrete')\n",
    "from utils_discrete import preprocess_data_discrete_identification, plot_results_discrete_identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and preprocess it\n",
    "data_path = 'data/burgers_shock.mat'\n",
    "\n",
    "N0 = 200\n",
    "N1 = 200\n",
    "noise = 0.0\n",
    "idx_t0 = 10\n",
    "idx_t1 = 90\n",
    "\n",
    "x, t, u_exact, lb, ub, dt, q, x0, u0, x1, u1, IRK_alphas, IRK_betas = preprocess_data_discrete_identification(data_path, idx_t0, idx_t1,\n",
    "                                                                                                                      N0, N1, noise)\n",
    "\n",
    "x0 = torch.tensor(x0, dtype = torch.float, requires_grad = True, device = device)\n",
    "u0 = torch.tensor(u0, dtype = torch.float, requires_grad = True, device = device)\n",
    "x1 = torch.tensor(x1, dtype = torch.float, requires_grad = True, device = device)\n",
    "u1 = torch.tensor(u1, dtype = torch.float, requires_grad = True, device = device)\n",
    "dt = torch.tensor(dt, dtype = torch.float, requires_grad = True, device = device)\n",
    "IRK_alphas = torch.tensor(IRK_alphas, dtype = torch.float, requires_grad = True, device = device)\n",
    "IRK_betas = torch.tensor(IRK_betas, dtype = torch.float, requires_grad = True, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(self, layers, lb, ub):\n",
    "        super().__init__()\n",
    "        self.lb = torch.tensor(lb, dtype = torch.float, device = device)\n",
    "        self.ub = torch.tensor(ub, dtype = torch.float, device = device)\n",
    "        \n",
    "        # Layer module\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Make the neural network\n",
    "        input_dim = layers[0]\n",
    "        for output_dim in layers[1:]:\n",
    "            self.layers.append(nn.Linear(input_dim, output_dim))\n",
    "            nn.init.xavier_normal_(self.layers[-1].weight)\n",
    "            input_dim = output_dim\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        x = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = torch.tanh(layer(x))\n",
    "\n",
    "        outputs = self.layers[-1](x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN:\n",
    "    def __init__(self, dt, x0, u0, x1, u1, lb, ub, IRK_alphas, IRK_betas, nu, layers = [2, 20, 20, 20, 1], lr = 1e-2, device = torch.device('cpu')):\n",
    "        self.nu = nu\n",
    "        \n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.u0 = u0\n",
    "        self.x1 = x1\n",
    "        self.u1 = u1\n",
    "        self.IRK_alphas = IRK_alphas\n",
    "        self.IRK_betas = IRK_betas\n",
    "        \n",
    "        self.net_u = NeuralNet(layers, lb, ub)\n",
    "        self.net_u.to(device)\n",
    "        \n",
    "        # Physical paramters to optimize\n",
    "        self.lmbda = nn.ParameterList()\n",
    "        self.lmbda.append(nn.Parameter(0.0*torch.ones(1, device = device)))\n",
    "        self.lmbda.append(nn.Parameter(-6.0*torch.ones(1, device = device)))\n",
    "        \n",
    "        params = list(self.lmbda) + list(self.net_u.parameters())\n",
    "        self.optimizer = torch.optim.Adam(params, lr = lr, betas=(0.9, 0.999))\n",
    "    \n",
    "    def forward_gradients(self, y, x):\n",
    "        temp1 = torch.ones(y.size(), device = device, requires_grad = True)\n",
    "        temp2 = torch.ones(x.size(), device = device)\n",
    "        \n",
    "        g = grad(y, x, grad_outputs = temp1, create_graph = True)[0]\n",
    "        dy = grad(g, temp1, grad_outputs = temp2, create_graph = True)[0]\n",
    "        \n",
    "        del temp1, temp2\n",
    "        \n",
    "        return dy\n",
    "    \n",
    "    def net_u0(self):       \n",
    "        u = self.net_u(self.x0)\n",
    "        \n",
    "        u_x  = self.forward_gradients(u, self.x0)\n",
    "        u_xx = self.forward_gradients(u_x, self.x0)\n",
    "        \n",
    "        \n",
    "        f = - self.lmbda[0]*u*u_x + torch.exp(self.lmbda[1])*u_xx\n",
    "        \n",
    "        u0 = u - self.dt*torch.matmul(f,self.IRK_alphas.T)\n",
    "        \n",
    "        return u0\n",
    "    \n",
    "    def net_u1(self):\n",
    "        u = self.net_u(self.x1)\n",
    "        \n",
    "        u_x  = self.forward_gradients(u, self.x1)\n",
    "        u_xx = self.forward_gradients(u_x, self.x1)\n",
    "        \n",
    "        \n",
    "        f = - self.lmbda[0]*u*u_x + torch.exp(self.lmbda[1])*u_xx\n",
    "        \n",
    "        u1 = u - self.dt*torch.matmul(f,(self.IRK_alphas-self.IRK_betas).T)\n",
    "        \n",
    "        return u1\n",
    "    \n",
    "    def loss_f(self, u0, u0_pred, u1, u1_pred):\n",
    "        return torch.mean(torch.square(u0-u0_pred)) + torch.mean(torch.square(u1-u1_pred))\n",
    "    \n",
    "    def optimizer_step(self):\n",
    "        # Zero the grads for the model paramters in the optimizer\n",
    "        self.optimizer.zero_grad()\n",
    "            \n",
    "        # Compute the losses and backpropagate the losses\n",
    "        loss = self.loss_f(self.u0, self.net_u0(), self.u1, self.net_u1())\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer one iteration with the given optimizer\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def fit(self, epochs = 1):\n",
    "        for epoch in range(epochs):\n",
    "            loss_value = self.optimizer_step()\n",
    "\n",
    "            if epoch % 100 == 99:\n",
    "                print(f'Epoch {epoch+1}: Training Loss = {loss_value}')\n",
    "            if epoch % 1000 == 999:\n",
    "                print(f'Lambda 1 = {self.lmbda[0].item()}, Lambda 2 = {torch.exp(self.lmbda[1]).item()}\\n')\n",
    "                \n",
    "    def predict(self):\n",
    "        return self.net_u0(), self.net_u1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn = PINN(dt, x0, u0, x1, u1, lb, ub, IRK_alphas, IRK_betas, nu = (0.01/np.pi), layers = [1, 50, 50, 50, q], lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Training Loss = 0.3353276252746582\n",
      "Epoch 200: Training Loss = 0.10609330981969833\n",
      "Epoch 300: Training Loss = 0.0689815878868103\n",
      "Epoch 400: Training Loss = 0.05847739428281784\n",
      "Epoch 500: Training Loss = 0.04970696568489075\n",
      "Epoch 600: Training Loss = 0.04084864258766174\n",
      "Epoch 700: Training Loss = 0.033506326377391815\n",
      "Epoch 800: Training Loss = 0.026966195553541183\n",
      "Epoch 900: Training Loss = 0.020770788192749023\n",
      "Epoch 1000: Training Loss = 0.01500019058585167\n",
      "Lambda 1 = 0.5542933344841003, Lambda 2 = 0.006957383826375008\n",
      "\n",
      "Epoch 1100: Training Loss = 0.009954620152711868\n",
      "Epoch 1200: Training Loss = 0.00671430304646492\n",
      "Epoch 1300: Training Loss = 0.004711630754172802\n",
      "Epoch 1400: Training Loss = 0.0034847483038902283\n",
      "Epoch 1500: Training Loss = 0.002707665553316474\n",
      "Epoch 1600: Training Loss = 0.002184429205954075\n",
      "Epoch 1700: Training Loss = 0.0018595880828797817\n",
      "Epoch 1800: Training Loss = 0.0027036915998905897\n",
      "Epoch 1900: Training Loss = 0.0012933930847793818\n",
      "Epoch 2000: Training Loss = 0.0011305988300591707\n",
      "Lambda 1 = 0.8841695189476013, Lambda 2 = 0.009289246052503586\n",
      "\n",
      "Epoch 2100: Training Loss = 0.0009850740898400545\n",
      "Epoch 2200: Training Loss = 0.0009020558791235089\n",
      "Epoch 2300: Training Loss = 0.0007903313962742686\n",
      "Epoch 2400: Training Loss = 0.0007696885732002556\n",
      "Epoch 2500: Training Loss = 0.0006603316869586706\n",
      "Epoch 2600: Training Loss = 0.0018235212191939354\n",
      "Epoch 2700: Training Loss = 0.0005724807269871235\n",
      "Epoch 2800: Training Loss = 0.0007596845971420407\n",
      "Epoch 2900: Training Loss = 0.0005459184758365154\n",
      "Epoch 3000: Training Loss = 0.0004795799031853676\n",
      "Lambda 1 = 0.9520111680030823, Lambda 2 = 0.008274275809526443\n",
      "\n",
      "Epoch 3100: Training Loss = 0.0004601309774443507\n",
      "Epoch 3200: Training Loss = 0.0004374794661998749\n",
      "Epoch 3300: Training Loss = 0.00043591816211119294\n",
      "Epoch 3400: Training Loss = 0.0004048930131830275\n",
      "Epoch 3500: Training Loss = 0.002669435925781727\n",
      "Epoch 3600: Training Loss = 0.0003781741834245622\n",
      "Epoch 3700: Training Loss = 0.0003636428155004978\n",
      "Epoch 3800: Training Loss = 0.00046003307215869427\n",
      "Epoch 3900: Training Loss = 0.00034218357177451253\n",
      "Epoch 4000: Training Loss = 0.0003297465154901147\n",
      "Lambda 1 = 0.9666417837142944, Lambda 2 = 0.00749567337334156\n",
      "\n",
      "Epoch 4100: Training Loss = 0.00034363684244453907\n",
      "Epoch 4200: Training Loss = 0.0003118737949989736\n",
      "Epoch 4300: Training Loss = 0.00030104551115073264\n",
      "Epoch 4400: Training Loss = 0.0002972144866362214\n",
      "Epoch 4500: Training Loss = 0.0002848387521225959\n",
      "Epoch 4600: Training Loss = 0.00030977121787145734\n",
      "Epoch 4700: Training Loss = 0.00026705884374678135\n",
      "Epoch 4800: Training Loss = 0.0002626190544106066\n",
      "Epoch 4900: Training Loss = 0.0002515339874662459\n",
      "Epoch 5000: Training Loss = 0.00024559497251175344\n",
      "Lambda 1 = 0.9724493622779846, Lambda 2 = 0.006965835578739643\n",
      "\n",
      "Epoch 5100: Training Loss = 0.00023679243167862296\n",
      "Epoch 5200: Training Loss = 0.00023339744075201452\n",
      "Epoch 5300: Training Loss = 0.00022390448430087417\n",
      "Epoch 5400: Training Loss = 0.0006240868242457509\n",
      "Epoch 5500: Training Loss = 0.0002135094255208969\n",
      "Epoch 5600: Training Loss = 0.00020596329704858363\n",
      "Epoch 5700: Training Loss = 0.0001988441654248163\n",
      "Epoch 5800: Training Loss = 0.00019566307310014963\n",
      "Epoch 5900: Training Loss = 0.00018804743012879044\n",
      "Epoch 6000: Training Loss = 0.0002497288805898279\n",
      "Lambda 1 = 0.9759941697120667, Lambda 2 = 0.006570605095475912\n",
      "\n",
      "Epoch 6100: Training Loss = 0.0001786116190487519\n",
      "Epoch 6200: Training Loss = 0.00017235838458873332\n",
      "Epoch 6300: Training Loss = 0.00016821021563373506\n",
      "Epoch 6400: Training Loss = 0.00025908995303325355\n",
      "Epoch 6500: Training Loss = 0.00015867716865614057\n",
      "Epoch 6600: Training Loss = 0.00016294335364364088\n",
      "Epoch 6700: Training Loss = 0.00015086928033269942\n",
      "Epoch 6800: Training Loss = 0.0001458392944186926\n",
      "Epoch 6900: Training Loss = 0.00014610013749916106\n",
      "Epoch 7000: Training Loss = 0.00013907524407841265\n",
      "Lambda 1 = 0.982912540435791, Lambda 2 = 0.00625211326405406\n",
      "\n",
      "Epoch 7100: Training Loss = 0.00013730533828493208\n",
      "Epoch 7200: Training Loss = 0.00013310338545124978\n",
      "Epoch 7300: Training Loss = 0.00012848683400079608\n",
      "Epoch 7400: Training Loss = 0.00012848418555222452\n",
      "Epoch 7500: Training Loss = 0.0001223267026944086\n",
      "Epoch 7600: Training Loss = 0.00012162573693785816\n",
      "Epoch 7700: Training Loss = 0.00011615049152169377\n",
      "Epoch 7800: Training Loss = 0.00011463008559076115\n",
      "Epoch 7900: Training Loss = 0.00011092629574704915\n",
      "Epoch 8000: Training Loss = 0.00012329193123150617\n",
      "Lambda 1 = 0.9852624535560608, Lambda 2 = 0.005996087100356817\n",
      "\n",
      "Epoch 8100: Training Loss = 0.0001060802023857832\n",
      "Epoch 8200: Training Loss = 0.00012089411757187918\n",
      "Epoch 8300: Training Loss = 0.00010190786269959062\n",
      "Epoch 8400: Training Loss = 0.00015452090883627534\n",
      "Epoch 8500: Training Loss = 9.832301293499768e-05\n",
      "Epoch 8600: Training Loss = 9.550089453114197e-05\n",
      "Epoch 8700: Training Loss = 0.0006147506646811962\n",
      "Epoch 8800: Training Loss = 9.251254959963262e-05\n",
      "Epoch 8900: Training Loss = 8.988688205135986e-05\n",
      "Epoch 9000: Training Loss = 9.06541827134788e-05\n",
      "Lambda 1 = 0.9881165027618408, Lambda 2 = 0.005784573033452034\n",
      "\n",
      "Epoch 9100: Training Loss = 8.647692447993904e-05\n",
      "Epoch 9200: Training Loss = 8.544606680516154e-05\n",
      "Epoch 9300: Training Loss = 8.311231795232743e-05\n",
      "Epoch 9400: Training Loss = 9.261291415896267e-05\n",
      "Epoch 9500: Training Loss = 8.04702503955923e-05\n",
      "Epoch 9600: Training Loss = 0.00011057803203584626\n",
      "Epoch 9700: Training Loss = 7.780781015753746e-05\n",
      "Epoch 9800: Training Loss = 8.875654748408124e-05\n",
      "Epoch 9900: Training Loss = 7.555356569355354e-05\n",
      "Epoch 10000: Training Loss = 7.356131391134113e-05\n",
      "Lambda 1 = 0.9901716113090515, Lambda 2 = 0.005607095081359148\n",
      "\n",
      "Epoch 10100: Training Loss = 7.621686381753534e-05\n",
      "Epoch 10200: Training Loss = 7.123335672076792e-05\n",
      "Epoch 10300: Training Loss = 0.00015214367886073887\n",
      "Epoch 10400: Training Loss = 6.955132994335145e-05\n",
      "Epoch 10500: Training Loss = 0.00034202137612737715\n",
      "Epoch 10600: Training Loss = 6.888163625262678e-05\n",
      "Epoch 10700: Training Loss = 6.5943444496952e-05\n",
      "Epoch 10800: Training Loss = 0.00012333964696153998\n",
      "Epoch 10900: Training Loss = 6.397443212335929e-05\n",
      "Epoch 11000: Training Loss = 0.00011355881724739447\n",
      "Lambda 1 = 0.9901666641235352, Lambda 2 = 0.0054552811197936535\n",
      "\n",
      "Epoch 11100: Training Loss = 6.196526373969391e-05\n",
      "Epoch 11200: Training Loss = 6.2576073105447e-05\n",
      "Epoch 11300: Training Loss = 6.016060069669038e-05\n",
      "Epoch 11400: Training Loss = 6.013601887389086e-05\n",
      "Epoch 11500: Training Loss = 5.84470180911012e-05\n",
      "Epoch 11600: Training Loss = 5.8856818213826045e-05\n",
      "Epoch 11700: Training Loss = 6.11093855695799e-05\n",
      "Epoch 11800: Training Loss = 5.8430385251995176e-05\n",
      "Epoch 11900: Training Loss = 5.55509323021397e-05\n",
      "Epoch 12000: Training Loss = 7.627898594364524e-05\n",
      "Lambda 1 = 0.9903172254562378, Lambda 2 = 0.005324507597833872\n",
      "\n",
      "Epoch 12100: Training Loss = 5.423414404504001e-05\n",
      "Epoch 12200: Training Loss = 0.0004075384058523923\n",
      "Epoch 12300: Training Loss = 5.59054606128484e-05\n",
      "Epoch 12400: Training Loss = 5.5499462177976966e-05\n",
      "Epoch 12500: Training Loss = 5.145315299159847e-05\n",
      "Epoch 12600: Training Loss = 5.175432306714356e-05\n",
      "Epoch 12700: Training Loss = 5.4366733820643276e-05\n",
      "Epoch 12800: Training Loss = 4.991167952539399e-05\n",
      "Epoch 12900: Training Loss = 4.9403934099245816e-05\n",
      "Epoch 13000: Training Loss = 4.831668047700077e-05\n",
      "Lambda 1 = 0.9920494556427002, Lambda 2 = 0.005208265967667103\n",
      "\n",
      "Epoch 13100: Training Loss = 5.163151945453137e-05\n",
      "Epoch 13200: Training Loss = 4.7123518015723675e-05\n",
      "Epoch 13300: Training Loss = 5.329570558387786e-05\n",
      "Epoch 13400: Training Loss = 4.625666406354867e-05\n",
      "Epoch 13500: Training Loss = 5.228804366197437e-05\n",
      "Epoch 13600: Training Loss = 7.203185668913648e-05\n",
      "Epoch 13700: Training Loss = 4.444961086846888e-05\n",
      "Epoch 13800: Training Loss = 4.471634019864723e-05\n",
      "Epoch 13900: Training Loss = 4.3628839193843305e-05\n",
      "Epoch 14000: Training Loss = 0.00014384377573151141\n",
      "Lambda 1 = 0.9922001361846924, Lambda 2 = 0.005108184181153774\n",
      "\n",
      "Epoch 14100: Training Loss = 4.252207145327702e-05\n",
      "Epoch 14200: Training Loss = 4.460019408725202e-05\n",
      "Epoch 14300: Training Loss = 7.280260615516454e-05\n",
      "Epoch 14400: Training Loss = 4.191576590528712e-05\n",
      "Epoch 14500: Training Loss = 4.0904051274992526e-05\n",
      "Epoch 14600: Training Loss = 0.0006797788664698601\n",
      "Epoch 14700: Training Loss = 4.013819125248119e-05\n",
      "Epoch 14800: Training Loss = 0.0004308927454985678\n",
      "Epoch 14900: Training Loss = 3.912892861990258e-05\n",
      "Epoch 15000: Training Loss = 3.928728256141767e-05\n",
      "Lambda 1 = 0.9925734996795654, Lambda 2 = 0.005018875002861023\n",
      "\n",
      "Epoch 15100: Training Loss = 0.0002691558329388499\n",
      "Epoch 15200: Training Loss = 3.798777470365167e-05\n",
      "Epoch 15300: Training Loss = 4.081627412233502e-05\n",
      "Epoch 15400: Training Loss = 4.005967639386654e-05\n",
      "Epoch 15500: Training Loss = 4.790147067978978e-05\n",
      "Epoch 15600: Training Loss = 3.723982081282884e-05\n",
      "Epoch 15700: Training Loss = 3.6172721593175083e-05\n",
      "Epoch 15800: Training Loss = 3.6233737773727626e-05\n",
      "Epoch 15900: Training Loss = 7.175978680606931e-05\n",
      "Epoch 16000: Training Loss = 4.0668244764674455e-05\n",
      "Lambda 1 = 0.9928357601165771, Lambda 2 = 0.004938466474413872\n",
      "\n",
      "Epoch 16100: Training Loss = 0.00020900051458738744\n",
      "Epoch 16200: Training Loss = 3.442943125264719e-05\n",
      "Epoch 16300: Training Loss = 3.784983346122317e-05\n",
      "Epoch 16400: Training Loss = 3.3755881304387e-05\n",
      "Epoch 16500: Training Loss = 3.4076110750902444e-05\n",
      "Epoch 16600: Training Loss = 3.549128450686112e-05\n",
      "Epoch 16700: Training Loss = 0.0003275616909377277\n",
      "Epoch 16800: Training Loss = 0.00012290995800867677\n",
      "Epoch 16900: Training Loss = 3.5698663850780576e-05\n",
      "Epoch 17000: Training Loss = 3.502608888084069e-05\n",
      "Lambda 1 = 0.9933384656906128, Lambda 2 = 0.004865042865276337\n",
      "\n",
      "Epoch 17100: Training Loss = 3.2008447305997834e-05\n",
      "Epoch 17200: Training Loss = 4.7295390686485916e-05\n",
      "Epoch 17300: Training Loss = 0.00013222052075434476\n",
      "Epoch 17400: Training Loss = 4.023702422273345e-05\n",
      "Epoch 17500: Training Loss = 5.493448406923562e-05\n",
      "Epoch 17600: Training Loss = 3.20388498948887e-05\n",
      "Epoch 17700: Training Loss = 3.1870396924205124e-05\n",
      "Epoch 17800: Training Loss = 6.665763794444501e-05\n",
      "Epoch 17900: Training Loss = 0.00010396269499324262\n",
      "Epoch 18000: Training Loss = 2.9303060728125274e-05\n",
      "Lambda 1 = 0.9937204122543335, Lambda 2 = 0.004799059126526117\n",
      "\n",
      "Epoch 18100: Training Loss = 2.9737857403233647e-05\n",
      "Epoch 18200: Training Loss = 3.1385134207084775e-05\n",
      "Epoch 18300: Training Loss = 2.8820410079788417e-05\n",
      "Epoch 18400: Training Loss = 3.0090028303675354e-05\n",
      "Epoch 18500: Training Loss = 2.888581366278231e-05\n",
      "Epoch 18600: Training Loss = 3.8022641092538834e-05\n",
      "Epoch 18700: Training Loss = 2.8893013222841546e-05\n",
      "Epoch 18800: Training Loss = 0.0006442980375140905\n",
      "Epoch 18900: Training Loss = 2.7304358809487894e-05\n",
      "Epoch 19000: Training Loss = 2.776554538286291e-05\n",
      "Lambda 1 = 0.9937466979026794, Lambda 2 = 0.004739090334624052\n",
      "\n",
      "Epoch 19100: Training Loss = 3.711602039402351e-05\n",
      "Epoch 19200: Training Loss = 5.328869156073779e-05\n",
      "Epoch 19300: Training Loss = 2.6461142624611966e-05\n",
      "Epoch 19400: Training Loss = 2.700105142139364e-05\n",
      "Epoch 19500: Training Loss = 0.0003093886189162731\n",
      "Epoch 19600: Training Loss = 0.00013757725537288934\n",
      "Epoch 19700: Training Loss = 2.579679130576551e-05\n",
      "Epoch 19800: Training Loss = 2.561262772360351e-05\n",
      "Epoch 19900: Training Loss = 5.39828761247918e-05\n",
      "Epoch 20000: Training Loss = 8.803214586805552e-05\n",
      "Lambda 1 = 0.9939929246902466, Lambda 2 = 0.004683415871113539\n",
      "\n",
      "Epoch 20100: Training Loss = 0.00042704393854364753\n",
      "Epoch 20200: Training Loss = 2.4813551135594025e-05\n",
      "Epoch 20300: Training Loss = 2.5092624127864838e-05\n",
      "Epoch 20400: Training Loss = 4.295363032724708e-05\n",
      "Epoch 20500: Training Loss = 2.4889297492336482e-05\n",
      "Epoch 20600: Training Loss = 2.7404121283325367e-05\n",
      "Epoch 20700: Training Loss = 4.417197487782687e-05\n",
      "Epoch 20800: Training Loss = 2.4077282432699576e-05\n",
      "Epoch 20900: Training Loss = 0.0002972601796500385\n",
      "Epoch 21000: Training Loss = 2.3577982574352063e-05\n",
      "Lambda 1 = 0.994233250617981, Lambda 2 = 0.004633021540939808\n",
      "\n",
      "Epoch 21100: Training Loss = 2.3368396796286106e-05\n",
      "Epoch 21200: Training Loss = 2.3684449843131006e-05\n",
      "Epoch 21300: Training Loss = 2.36656196648255e-05\n",
      "Epoch 21400: Training Loss = 2.498167668818496e-05\n",
      "Epoch 21500: Training Loss = 6.13289448665455e-05\n",
      "Epoch 21600: Training Loss = 2.730718915699981e-05\n",
      "Epoch 21700: Training Loss = 4.455899397726171e-05\n",
      "Epoch 21800: Training Loss = 2.2271735360845923e-05\n",
      "Epoch 21900: Training Loss = 7.665947487112135e-05\n",
      "Epoch 22000: Training Loss = 3.6624856875278056e-05\n",
      "Lambda 1 = 0.9943251013755798, Lambda 2 = 0.004585370421409607\n",
      "\n",
      "Epoch 22100: Training Loss = 2.2852676920592785e-05\n",
      "Epoch 22200: Training Loss = 2.4886241590138525e-05\n",
      "Epoch 22300: Training Loss = 3.0825656722299755e-05\n",
      "Epoch 22400: Training Loss = 2.1410938643384725e-05\n",
      "Epoch 22500: Training Loss = 2.2342221200233325e-05\n",
      "Epoch 22600: Training Loss = 2.619072620291263e-05\n",
      "Epoch 22700: Training Loss = 2.1679885321645997e-05\n",
      "Epoch 22800: Training Loss = 2.1594958525383845e-05\n",
      "Epoch 22900: Training Loss = 2.4900404241634533e-05\n",
      "Epoch 23000: Training Loss = 2.1947238565189764e-05\n",
      "Lambda 1 = 0.9945368766784668, Lambda 2 = 0.004541580565273762\n",
      "\n",
      "Epoch 23100: Training Loss = 2.070312802970875e-05\n",
      "Epoch 23200: Training Loss = 2.4357406800845638e-05\n",
      "Epoch 23300: Training Loss = 2.0325502191553824e-05\n",
      "Epoch 23400: Training Loss = 3.859179560095072e-05\n",
      "Epoch 23500: Training Loss = 2.0046089048264548e-05\n",
      "Epoch 23600: Training Loss = 2.043091990344692e-05\n",
      "Epoch 23700: Training Loss = 2.1645284505211748e-05\n",
      "Epoch 23800: Training Loss = 1.9763752789003775e-05\n",
      "Epoch 23900: Training Loss = 1.9925753804272972e-05\n",
      "Epoch 24000: Training Loss = 1.9695031369337812e-05\n",
      "Lambda 1 = 0.9946802854537964, Lambda 2 = 0.004501029849052429\n",
      "\n",
      "Epoch 24100: Training Loss = 2.1399355318862945e-05\n",
      "Epoch 24200: Training Loss = 1.9490558770485222e-05\n",
      "Epoch 24300: Training Loss = 2.0238647266523913e-05\n",
      "Epoch 24400: Training Loss = 2.92610602627974e-05\n",
      "Epoch 24500: Training Loss = 7.751492375973612e-05\n",
      "Epoch 24600: Training Loss = 2.197905268985778e-05\n",
      "Epoch 24700: Training Loss = 3.836516407318413e-05\n",
      "Epoch 24800: Training Loss = 2.5382094463566318e-05\n",
      "Epoch 24900: Training Loss = 0.00011286501830909401\n",
      "Epoch 25000: Training Loss = 0.00026823621010407805\n",
      "Lambda 1 = 0.9948367476463318, Lambda 2 = 0.004462200682610273\n",
      "\n",
      "Epoch 25100: Training Loss = 0.00017974412185139954\n",
      "Epoch 25200: Training Loss = 6.338391540339217e-05\n",
      "Epoch 25300: Training Loss = 2.9043883841950446e-05\n",
      "Epoch 25400: Training Loss = 1.825599611038342e-05\n",
      "Epoch 25500: Training Loss = 1.8578119124867953e-05\n",
      "Epoch 25600: Training Loss = 7.489921699743718e-05\n",
      "Epoch 25700: Training Loss = 2.426205901429057e-05\n",
      "Epoch 25800: Training Loss = 3.936203574994579e-05\n",
      "Epoch 25900: Training Loss = 3.88643384212628e-05\n",
      "Epoch 26000: Training Loss = 0.0003273909096606076\n",
      "Lambda 1 = 0.995019257068634, Lambda 2 = 0.004425432533025742\n",
      "\n",
      "Epoch 26100: Training Loss = 9.822040738072246e-05\n",
      "Epoch 26200: Training Loss = 1.7400729120709002e-05\n",
      "Epoch 26300: Training Loss = 1.880872150650248e-05\n",
      "Epoch 26400: Training Loss = 4.386767614050768e-05\n",
      "Epoch 26500: Training Loss = 2.2326204998535104e-05\n",
      "Epoch 26600: Training Loss = 2.5342622393509373e-05\n",
      "Epoch 26700: Training Loss = 1.6900703485589474e-05\n",
      "Epoch 26800: Training Loss = 1.6981064618448727e-05\n",
      "Epoch 26900: Training Loss = 2.0009742002002895e-05\n",
      "Epoch 27000: Training Loss = 2.014788697124459e-05\n",
      "Lambda 1 = 0.9950827956199646, Lambda 2 = 0.004391584079712629\n",
      "\n",
      "Epoch 27100: Training Loss = 1.9015198631677777e-05\n",
      "Epoch 27200: Training Loss = 1.667331343924161e-05\n",
      "Epoch 27300: Training Loss = 1.9876282749464735e-05\n",
      "Epoch 27400: Training Loss = 3.854827082250267e-05\n",
      "Epoch 27500: Training Loss = 1.6569454601267353e-05\n",
      "Epoch 27600: Training Loss = 1.7270507669309154e-05\n",
      "Epoch 27700: Training Loss = 4.9415793910156935e-05\n",
      "Epoch 27800: Training Loss = 6.836154352640733e-05\n",
      "Epoch 27900: Training Loss = 0.00021700386423617601\n",
      "Epoch 28000: Training Loss = 9.324843995273113e-05\n",
      "Lambda 1 = 0.9952639937400818, Lambda 2 = 0.004359879530966282\n",
      "\n",
      "Epoch 28100: Training Loss = 2.3925451387185603e-05\n",
      "Epoch 28200: Training Loss = 2.472784399287775e-05\n",
      "Epoch 28300: Training Loss = 1.8640595953911543e-05\n",
      "Epoch 28400: Training Loss = 1.7220942027051933e-05\n",
      "Epoch 28500: Training Loss = 0.0003287607687525451\n",
      "Epoch 28600: Training Loss = 3.226944318157621e-05\n",
      "Epoch 28700: Training Loss = 2.2641264877165668e-05\n",
      "Epoch 28800: Training Loss = 6.996275624260306e-05\n",
      "Epoch 28900: Training Loss = 7.855617150198668e-05\n",
      "Epoch 29000: Training Loss = 3.66712556569837e-05\n",
      "Lambda 1 = 0.9952490329742432, Lambda 2 = 0.00432930001989007\n",
      "\n",
      "Epoch 29100: Training Loss = 1.5517203792114742e-05\n",
      "Epoch 29200: Training Loss = 5.767806214862503e-05\n",
      "Epoch 29300: Training Loss = 1.8018054106505588e-05\n",
      "Epoch 29400: Training Loss = 1.786906796041876e-05\n",
      "Epoch 29500: Training Loss = 1.590324245626107e-05\n",
      "Epoch 29600: Training Loss = 5.316752503858879e-05\n",
      "Epoch 29700: Training Loss = 2.8941034543095157e-05\n",
      "Epoch 29800: Training Loss = 1.649654223001562e-05\n",
      "Epoch 29900: Training Loss = 5.796074037789367e-05\n",
      "Epoch 30000: Training Loss = 1.828244603530038e-05\n",
      "Lambda 1 = 0.9955593943595886, Lambda 2 = 0.004300433676689863\n",
      "\n",
      "Epoch 30100: Training Loss = 6.997644231887534e-05\n",
      "Epoch 30200: Training Loss = 0.00029693852411583066\n",
      "Epoch 30300: Training Loss = 1.4544782970915549e-05\n",
      "Epoch 30400: Training Loss = 2.112020411004778e-05\n",
      "Epoch 30500: Training Loss = 1.942869675986003e-05\n",
      "Epoch 30600: Training Loss = 3.684472176246345e-05\n",
      "Epoch 30700: Training Loss = 1.467904348828597e-05\n",
      "Epoch 30800: Training Loss = 2.04746684175916e-05\n",
      "Epoch 30900: Training Loss = 3.239883153582923e-05\n",
      "Epoch 31000: Training Loss = 0.000166745187016204\n",
      "Lambda 1 = 0.9955599904060364, Lambda 2 = 0.004272645805031061\n",
      "\n",
      "Epoch 31100: Training Loss = 8.418549259658903e-05\n",
      "Epoch 31200: Training Loss = 2.6863192033488303e-05\n",
      "Epoch 31300: Training Loss = 2.632395626278594e-05\n",
      "Epoch 31400: Training Loss = 1.4049159290152602e-05\n",
      "Epoch 31500: Training Loss = 0.00010120544175151736\n",
      "Epoch 31600: Training Loss = 1.8447415641276166e-05\n",
      "Epoch 31700: Training Loss = 6.977701559662819e-05\n",
      "Epoch 31800: Training Loss = 3.074480628129095e-05\n",
      "Epoch 31900: Training Loss = 2.7721445803763345e-05\n",
      "Epoch 32000: Training Loss = 3.8857790059410036e-05\n",
      "Lambda 1 = 0.9955978393554688, Lambda 2 = 0.004246841184794903\n",
      "\n",
      "Epoch 32100: Training Loss = 9.845992462942377e-05\n",
      "Epoch 32200: Training Loss = 0.00012229675485286862\n",
      "Epoch 32300: Training Loss = 5.139724089531228e-05\n",
      "Epoch 32400: Training Loss = 1.3388431398198009e-05\n",
      "Epoch 32500: Training Loss = 6.202961958479136e-05\n",
      "Epoch 32600: Training Loss = 1.7488278899691068e-05\n",
      "Epoch 32700: Training Loss = 1.3166059943614528e-05\n",
      "Epoch 32800: Training Loss = 8.854144834913313e-05\n",
      "Epoch 32900: Training Loss = 4.7904632083373144e-05\n",
      "Epoch 33000: Training Loss = 1.5706360500189476e-05\n",
      "Lambda 1 = 0.9957960844039917, Lambda 2 = 0.004221726208925247\n",
      "\n",
      "Epoch 33100: Training Loss = 4.113031172892079e-05\n",
      "Epoch 33200: Training Loss = 1.2830330888391472e-05\n",
      "Epoch 33300: Training Loss = 1.7313563148491085e-05\n",
      "Epoch 33400: Training Loss = 1.2451277143554762e-05\n",
      "Epoch 33500: Training Loss = 1.6422965927631594e-05\n",
      "Epoch 33600: Training Loss = 2.3315649741562083e-05\n",
      "Epoch 33700: Training Loss = 1.3403203411144204e-05\n",
      "Epoch 33800: Training Loss = 1.637034074519761e-05\n",
      "Epoch 33900: Training Loss = 1.3986458725412376e-05\n",
      "Epoch 34000: Training Loss = 1.2221629731357098e-05\n",
      "Lambda 1 = 0.9958667159080505, Lambda 2 = 0.0041981907561421394\n",
      "\n",
      "Epoch 34100: Training Loss = 1.3513133126252797e-05\n",
      "Epoch 34200: Training Loss = 1.2053520549670793e-05\n",
      "Epoch 34300: Training Loss = 1.4599825590266846e-05\n",
      "Epoch 34400: Training Loss = 1.9980030629085377e-05\n",
      "Epoch 34500: Training Loss = 2.1316849597496912e-05\n",
      "Epoch 34600: Training Loss = 1.3557163583755028e-05\n",
      "Epoch 34700: Training Loss = 2.0653995306929573e-05\n",
      "Epoch 34800: Training Loss = 1.6230200344580226e-05\n",
      "Epoch 34900: Training Loss = 2.363119347137399e-05\n",
      "Epoch 35000: Training Loss = 2.1143845515325665e-05\n",
      "Lambda 1 = 0.9958990216255188, Lambda 2 = 0.00417542178183794\n",
      "\n",
      "Epoch 35100: Training Loss = 1.485215216234792e-05\n",
      "Epoch 35200: Training Loss = 3.6735873436555266e-05\n",
      "Epoch 35300: Training Loss = 1.5118459486984648e-05\n",
      "Epoch 35400: Training Loss = 1.4027878933120519e-05\n",
      "Epoch 35500: Training Loss = 1.7106018276535906e-05\n",
      "Epoch 35600: Training Loss = 2.89914842142025e-05\n",
      "Epoch 35700: Training Loss = 0.00010803551413118839\n",
      "Epoch 35800: Training Loss = 2.678860619198531e-05\n",
      "Epoch 35900: Training Loss = 3.071074388572015e-05\n",
      "Epoch 36000: Training Loss = 4.8843394324649125e-05\n",
      "Lambda 1 = 0.9959354996681213, Lambda 2 = 0.0041534374468028545\n",
      "\n",
      "Epoch 36100: Training Loss = 1.5342107872129418e-05\n",
      "Epoch 36200: Training Loss = 1.149310810433235e-05\n",
      "Epoch 36300: Training Loss = 1.8399503460386768e-05\n",
      "Epoch 36400: Training Loss = 4.972450187779032e-05\n",
      "Epoch 36500: Training Loss = 1.202589919557795e-05\n",
      "Epoch 36600: Training Loss = 2.485407458152622e-05\n",
      "Epoch 36700: Training Loss = 1.1312010428810026e-05\n",
      "Epoch 36800: Training Loss = 1.3609474990516901e-05\n",
      "Epoch 36900: Training Loss = 1.1032052498194389e-05\n",
      "Epoch 37000: Training Loss = 1.080918991647195e-05\n",
      "Lambda 1 = 0.9961031675338745, Lambda 2 = 0.004132944159209728\n",
      "\n",
      "Epoch 37100: Training Loss = 4.6184391976566985e-05\n",
      "Epoch 37200: Training Loss = 1.0738739547377918e-05\n",
      "Epoch 37300: Training Loss = 1.0938050763797946e-05\n",
      "Epoch 37400: Training Loss = 1.2500373486545868e-05\n",
      "Epoch 37500: Training Loss = 2.4500703148078173e-05\n",
      "Epoch 37600: Training Loss = 1.1158468623762019e-05\n",
      "Epoch 37700: Training Loss = 6.0856833442812786e-05\n",
      "Epoch 37800: Training Loss = 1.7896905774250627e-05\n",
      "Epoch 37900: Training Loss = 2.0335884983069263e-05\n",
      "Epoch 38000: Training Loss = 1.0491098692000378e-05\n",
      "Lambda 1 = 0.9962173104286194, Lambda 2 = 0.0041127740405499935\n",
      "\n",
      "Epoch 38100: Training Loss = 1.0413064956082962e-05\n",
      "Epoch 38200: Training Loss = 1.1538577382452786e-05\n",
      "Epoch 38300: Training Loss = 1.4927242773410399e-05\n",
      "Epoch 38400: Training Loss = 6.951300747459754e-05\n",
      "Epoch 38500: Training Loss = 1.0566646778897848e-05\n",
      "Epoch 38600: Training Loss = 1.2038651220791508e-05\n",
      "Epoch 38700: Training Loss = 8.505208825226873e-05\n",
      "Epoch 38800: Training Loss = 1.3336921256268397e-05\n",
      "Epoch 38900: Training Loss = 1.1691103281918913e-05\n",
      "Epoch 39000: Training Loss = 2.465161742293276e-05\n",
      "Lambda 1 = 0.9962689280509949, Lambda 2 = 0.004093625117093325\n",
      "\n",
      "Epoch 39100: Training Loss = 2.493622014299035e-05\n",
      "Epoch 39200: Training Loss = 0.0003614806628320366\n",
      "Epoch 39300: Training Loss = 2.347280315007083e-05\n",
      "Epoch 39400: Training Loss = 4.515255204751156e-05\n",
      "Epoch 39500: Training Loss = 1.5674730093451217e-05\n",
      "Epoch 39600: Training Loss = 1.0355026461184025e-05\n",
      "Epoch 39700: Training Loss = 2.2141568479128182e-05\n",
      "Epoch 39800: Training Loss = 3.052535612368956e-05\n",
      "Epoch 39900: Training Loss = 1.4998717233538628e-05\n",
      "Epoch 40000: Training Loss = 1.987932228075806e-05\n",
      "Lambda 1 = 0.9962941408157349, Lambda 2 = 0.004075183067470789\n",
      "\n",
      "Epoch 40100: Training Loss = 1.1345557140884921e-05\n",
      "Epoch 40200: Training Loss = 3.8455240428447723e-05\n",
      "Epoch 40300: Training Loss = 1.1951809028687421e-05\n",
      "Epoch 40400: Training Loss = 4.3977666791761294e-05\n",
      "Epoch 40500: Training Loss = 2.7334539481671527e-05\n",
      "Epoch 40600: Training Loss = 1.2517486538854428e-05\n",
      "Epoch 40700: Training Loss = 1.6945707102422602e-05\n",
      "Epoch 40800: Training Loss = 1.0780018783407286e-05\n",
      "Epoch 40900: Training Loss = 4.488343984121457e-05\n",
      "Epoch 41000: Training Loss = 8.190612425096333e-05\n",
      "Lambda 1 = 0.9963428974151611, Lambda 2 = 0.004057333339005709\n",
      "\n",
      "Epoch 41100: Training Loss = 1.120711112889694e-05\n",
      "Epoch 41200: Training Loss = 2.4870294510037638e-05\n",
      "Epoch 41300: Training Loss = 9.831139323068783e-06\n",
      "Epoch 41400: Training Loss = 1.1509804608067498e-05\n",
      "Epoch 41500: Training Loss = 1.2430867172952276e-05\n",
      "Epoch 41600: Training Loss = 5.767546099377796e-05\n",
      "Epoch 41700: Training Loss = 1.168202652479522e-05\n",
      "Epoch 41800: Training Loss = 1.0344362635805737e-05\n",
      "Epoch 41900: Training Loss = 0.00010252623906126246\n",
      "Epoch 42000: Training Loss = 1.4416547855944373e-05\n",
      "Lambda 1 = 0.9963829517364502, Lambda 2 = 0.004040214698761702\n",
      "\n",
      "Epoch 42100: Training Loss = 1.3933857189840637e-05\n",
      "Epoch 42200: Training Loss = 9.586270607542247e-06\n",
      "Epoch 42300: Training Loss = 9.119901733356528e-06\n",
      "Epoch 42400: Training Loss = 9.39055644266773e-06\n",
      "Epoch 42500: Training Loss = 2.0825478713959455e-05\n",
      "Epoch 42600: Training Loss = 5.5276188504649326e-05\n",
      "Epoch 42700: Training Loss = 9.188959666062146e-06\n",
      "Epoch 42800: Training Loss = 5.831618909724057e-05\n",
      "Epoch 42900: Training Loss = 6.258462963160127e-05\n",
      "Epoch 43000: Training Loss = 3.7915022403467447e-05\n",
      "Lambda 1 = 0.9965439438819885, Lambda 2 = 0.004023298621177673\n",
      "\n",
      "Epoch 43100: Training Loss = 1.0671508789528161e-05\n",
      "Epoch 43200: Training Loss = 1.1725644071702845e-05\n",
      "Epoch 43300: Training Loss = 2.1583984562312253e-05\n",
      "Epoch 43400: Training Loss = 3.5522476537153125e-05\n",
      "Epoch 43500: Training Loss = 8.784274541540071e-06\n",
      "Epoch 43600: Training Loss = 2.7811218387796544e-05\n",
      "Epoch 43700: Training Loss = 1.5564872228424065e-05\n",
      "Epoch 43800: Training Loss = 1.0804702469613403e-05\n",
      "Epoch 43900: Training Loss = 9.214215242536739e-05\n",
      "Epoch 44000: Training Loss = 8.458237061859109e-06\n",
      "Lambda 1 = 0.9965724349021912, Lambda 2 = 0.004007881041616201\n",
      "\n",
      "Epoch 44100: Training Loss = 1.012397842714563e-05\n",
      "Epoch 44200: Training Loss = 8.420646736340132e-06\n",
      "Epoch 44300: Training Loss = 9.91973138297908e-06\n",
      "Epoch 44400: Training Loss = 3.924066186300479e-05\n",
      "Epoch 44500: Training Loss = 9.416640750714578e-06\n",
      "Epoch 44600: Training Loss = 3.127011586911976e-05\n",
      "Epoch 44700: Training Loss = 1.8164253560826182e-05\n",
      "Epoch 44800: Training Loss = 1.0133033356396481e-05\n",
      "Epoch 44900: Training Loss = 9.79815740720369e-06\n",
      "Epoch 45000: Training Loss = 1.4724211723660119e-05\n",
      "Lambda 1 = 0.9965892434120178, Lambda 2 = 0.003992470912635326\n",
      "\n",
      "Epoch 45100: Training Loss = 5.352935113478452e-05\n",
      "Epoch 45200: Training Loss = 8.562983566662297e-06\n",
      "Epoch 45300: Training Loss = 9.58278360485565e-06\n",
      "Epoch 45400: Training Loss = 9.80327786237467e-06\n",
      "Epoch 45500: Training Loss = 5.527563553187065e-05\n",
      "Epoch 45600: Training Loss = 5.782027801615186e-05\n",
      "Epoch 45700: Training Loss = 2.592791133793071e-05\n",
      "Epoch 45800: Training Loss = 2.5929806724889204e-05\n",
      "Epoch 45900: Training Loss = 2.7428701287135482e-05\n",
      "Epoch 46000: Training Loss = 7.484560774173588e-05\n",
      "Lambda 1 = 0.996616542339325, Lambda 2 = 0.0039777266792953014\n",
      "\n",
      "Epoch 46100: Training Loss = 1.179770606540842e-05\n",
      "Epoch 46200: Training Loss = 9.157016393146478e-06\n",
      "Epoch 46300: Training Loss = 6.840322021162137e-05\n",
      "Epoch 46400: Training Loss = 1.6524922102689743e-05\n",
      "Epoch 46500: Training Loss = 8.472096851619426e-06\n",
      "Epoch 46600: Training Loss = 4.4340187741909176e-05\n",
      "Epoch 46700: Training Loss = 8.644103218102828e-06\n",
      "Epoch 46800: Training Loss = 8.051466465985868e-06\n",
      "Epoch 46900: Training Loss = 4.142734542256221e-05\n",
      "Epoch 47000: Training Loss = 9.88542797131231e-06\n",
      "Lambda 1 = 0.9967513084411621, Lambda 2 = 0.003963230177760124\n",
      "\n",
      "Epoch 47100: Training Loss = 7.689543963351753e-06\n",
      "Epoch 47200: Training Loss = 3.537218435667455e-05\n",
      "Epoch 47300: Training Loss = 8.294497092720121e-05\n",
      "Epoch 47400: Training Loss = 0.00016779144061729312\n",
      "Epoch 47500: Training Loss = 0.00010716529504861683\n",
      "Epoch 47600: Training Loss = 7.56514236854855e-06\n",
      "Epoch 47700: Training Loss = 2.671430047485046e-05\n",
      "Epoch 47800: Training Loss = 1.0750383808044717e-05\n",
      "Epoch 47900: Training Loss = 5.77439641347155e-05\n",
      "Epoch 48000: Training Loss = 0.00011053246271330863\n",
      "Lambda 1 = 0.9968264102935791, Lambda 2 = 0.003949081525206566\n",
      "\n",
      "Epoch 48100: Training Loss = 9.961547220882494e-06\n",
      "Epoch 48200: Training Loss = 2.1449402993312106e-05\n",
      "Epoch 48300: Training Loss = 1.831396912166383e-05\n",
      "Epoch 48400: Training Loss = 1.1072816050727852e-05\n",
      "Epoch 48500: Training Loss = 2.1789845050079748e-05\n",
      "Epoch 48600: Training Loss = 1.591128602740355e-05\n",
      "Epoch 48700: Training Loss = 3.240164733142592e-05\n",
      "Epoch 48800: Training Loss = 1.4818009731243365e-05\n",
      "Epoch 48900: Training Loss = 2.5109704438364133e-05\n",
      "Epoch 49000: Training Loss = 8.479356438328978e-06\n",
      "Lambda 1 = 0.9968164563179016, Lambda 2 = 0.00393596151843667\n",
      "\n",
      "Epoch 49100: Training Loss = 7.195820944616571e-05\n",
      "Epoch 49200: Training Loss = 7.725001341896132e-05\n",
      "Epoch 49300: Training Loss = 3.1257804948836565e-05\n",
      "Epoch 49400: Training Loss = 2.1787633158965036e-05\n",
      "Epoch 49500: Training Loss = 7.2199645728687756e-06\n",
      "Epoch 49600: Training Loss = 7.577753876830684e-06\n",
      "Epoch 49700: Training Loss = 7.794045814080164e-06\n",
      "Epoch 49800: Training Loss = 1.8210717826150358e-05\n",
      "Epoch 49900: Training Loss = 4.3067724618595093e-05\n",
      "Epoch 50000: Training Loss = 1.630804399610497e-05\n",
      "Lambda 1 = 0.9968749284744263, Lambda 2 = 0.003923036623746157\n",
      "\n",
      "Epoch 50100: Training Loss = 1.1466256182757206e-05\n",
      "Epoch 50200: Training Loss = 7.785671186866239e-06\n",
      "Epoch 50300: Training Loss = 9.654405403125565e-06\n",
      "Epoch 50400: Training Loss = 1.0133280738955364e-05\n",
      "Epoch 50500: Training Loss = 1.636305751162581e-05\n",
      "Epoch 50600: Training Loss = 2.2063299184083007e-05\n",
      "Epoch 50700: Training Loss = 2.574890822870657e-05\n",
      "Epoch 50800: Training Loss = 1.0648899660736788e-05\n",
      "Epoch 50900: Training Loss = 6.977133307373151e-05\n",
      "Epoch 51000: Training Loss = 7.716630534559954e-06\n",
      "Lambda 1 = 0.9969329237937927, Lambda 2 = 0.003910523373633623\n",
      "\n",
      "Epoch 51100: Training Loss = 2.0140807464485988e-05\n",
      "Epoch 51200: Training Loss = 1.3800005035591312e-05\n",
      "Epoch 51300: Training Loss = 9.166393283521757e-06\n",
      "Epoch 51400: Training Loss = 6.8005401772097684e-06\n",
      "Epoch 51500: Training Loss = 6.84360657032812e-06\n",
      "Epoch 51600: Training Loss = 1.8234801245853305e-05\n",
      "Epoch 51700: Training Loss = 2.1806157747050747e-05\n",
      "Epoch 51800: Training Loss = 1.9905590306734666e-05\n",
      "Epoch 51900: Training Loss = 9.157030035567004e-06\n",
      "Epoch 52000: Training Loss = 9.040483746503014e-06\n",
      "Lambda 1 = 0.9969356060028076, Lambda 2 = 0.003898375201970339\n",
      "\n",
      "Epoch 52100: Training Loss = 7.1897247835295275e-06\n",
      "Epoch 52200: Training Loss = 3.64278384950012e-05\n",
      "Epoch 52300: Training Loss = 2.694030263228342e-05\n",
      "Epoch 52400: Training Loss = 9.946053978637792e-06\n",
      "Epoch 52500: Training Loss = 6.800025585107505e-05\n",
      "Epoch 52600: Training Loss = 1.2275401786610018e-05\n",
      "Epoch 52700: Training Loss = 1.7181668226839975e-05\n",
      "Epoch 52800: Training Loss = 0.0001400242035742849\n",
      "Epoch 52900: Training Loss = 3.317210575914942e-05\n",
      "Epoch 53000: Training Loss = 6.51167101750616e-06\n",
      "Lambda 1 = 0.9970048666000366, Lambda 2 = 0.003886596532538533\n",
      "\n",
      "Epoch 53100: Training Loss = 5.096699169371277e-05\n",
      "Epoch 53200: Training Loss = 7.214284778456204e-06\n",
      "Epoch 53300: Training Loss = 2.8674709028564394e-05\n",
      "Epoch 53400: Training Loss = 6.553635103045963e-06\n",
      "Epoch 53500: Training Loss = 7.757065759506077e-06\n",
      "Epoch 53600: Training Loss = 8.098720172711182e-06\n",
      "Epoch 53700: Training Loss = 5.827028144267388e-05\n",
      "Epoch 53800: Training Loss = 8.4794701251667e-06\n",
      "Epoch 53900: Training Loss = 2.7187932573724538e-05\n",
      "Epoch 54000: Training Loss = 4.587852890836075e-05\n",
      "Lambda 1 = 0.9971223473548889, Lambda 2 = 0.0038749107625335455\n",
      "\n",
      "Epoch 54100: Training Loss = 2.3531112674390897e-05\n",
      "Epoch 54200: Training Loss = 0.00015514358528889716\n",
      "Epoch 54300: Training Loss = 6.3577117543900385e-06\n",
      "Epoch 54400: Training Loss = 6.299686447164277e-06\n",
      "Epoch 54500: Training Loss = 8.027142030186951e-06\n",
      "Epoch 54600: Training Loss = 6.401605787687004e-06\n",
      "Epoch 54700: Training Loss = 9.079044684767723e-05\n",
      "Epoch 54800: Training Loss = 4.3886462663067505e-05\n",
      "Epoch 54900: Training Loss = 2.275418592034839e-05\n",
      "Epoch 55000: Training Loss = 1.081027676264057e-05\n",
      "Lambda 1 = 0.9971168041229248, Lambda 2 = 0.0038640338461846113\n",
      "\n",
      "Epoch 55100: Training Loss = 2.83742028841516e-05\n",
      "Epoch 55200: Training Loss = 6.162225872685667e-06\n",
      "Epoch 55300: Training Loss = 1.2429580237949267e-05\n",
      "Epoch 55400: Training Loss = 8.58333078213036e-06\n",
      "Epoch 55500: Training Loss = 6.197123184392694e-06\n",
      "Epoch 55600: Training Loss = 1.920285285450518e-05\n",
      "Epoch 55700: Training Loss = 3.099296009168029e-05\n",
      "Epoch 55800: Training Loss = 5.6181092077167705e-05\n",
      "Epoch 55900: Training Loss = 1.170973882835824e-05\n",
      "Epoch 56000: Training Loss = 1.9108814740320668e-05\n",
      "Lambda 1 = 0.9971785545349121, Lambda 2 = 0.0038533234037458897\n",
      "\n",
      "Epoch 56100: Training Loss = 6.52120215818286e-05\n",
      "Epoch 56200: Training Loss = 6.662466148554813e-06\n",
      "Epoch 56300: Training Loss = 6.007860974932555e-06\n",
      "Epoch 56400: Training Loss = 6.26960099907592e-05\n",
      "Epoch 56500: Training Loss = 3.043130891455803e-05\n",
      "Epoch 56600: Training Loss = 6.693793238810031e-06\n",
      "Epoch 56700: Training Loss = 1.6789548681117594e-05\n",
      "Epoch 56800: Training Loss = 8.538570909877308e-06\n",
      "Epoch 56900: Training Loss = 7.510338036809117e-06\n",
      "Epoch 57000: Training Loss = 5.88837792747654e-06\n",
      "Lambda 1 = 0.997242271900177, Lambda 2 = 0.003842824138700962\n",
      "\n",
      "Epoch 57100: Training Loss = 6.1066466514603235e-06\n",
      "Epoch 57200: Training Loss = 6.9969846663298085e-06\n",
      "Epoch 57300: Training Loss = 1.9219338355469517e-05\n",
      "Epoch 57400: Training Loss = 2.5521232601022348e-05\n",
      "Epoch 57500: Training Loss = 5.920039257034659e-06\n",
      "Epoch 57600: Training Loss = 8.690172762726434e-06\n",
      "Epoch 57700: Training Loss = 5.599050928140059e-05\n",
      "Epoch 57800: Training Loss = 6.517970177810639e-06\n",
      "Epoch 57900: Training Loss = 3.4675103961490095e-05\n",
      "Epoch 58000: Training Loss = 0.0001507570268586278\n",
      "Lambda 1 = 0.9974305033683777, Lambda 2 = 0.0038322783075273037\n",
      "\n",
      "Epoch 58100: Training Loss = 9.911115739669185e-06\n",
      "Epoch 58200: Training Loss = 5.792829688289203e-05\n",
      "Epoch 58300: Training Loss = 5.6208177738881204e-06\n",
      "Epoch 58400: Training Loss = 1.6555328329559416e-05\n",
      "Epoch 58500: Training Loss = 7.020570137683535e-06\n",
      "Epoch 58600: Training Loss = 6.295925231825095e-06\n",
      "Epoch 58700: Training Loss = 1.4744991858606227e-05\n",
      "Epoch 58800: Training Loss = 6.942594154679682e-06\n",
      "Epoch 58900: Training Loss = 1.4762088540010154e-05\n",
      "Epoch 59000: Training Loss = 6.946587745915167e-06\n",
      "Lambda 1 = 0.9973220825195312, Lambda 2 = 0.0038230321370065212\n",
      "\n",
      "Epoch 59100: Training Loss = 1.28196752484655e-05\n",
      "Epoch 59200: Training Loss = 9.875238902168348e-06\n",
      "Epoch 59300: Training Loss = 3.2229003409156576e-05\n",
      "Epoch 59400: Training Loss = 0.00012775998038705438\n",
      "Epoch 59500: Training Loss = 5.462513399834279e-06\n",
      "Epoch 59600: Training Loss = 0.00013569975271821022\n",
      "Epoch 59700: Training Loss = 1.3945678801974282e-05\n",
      "Epoch 59800: Training Loss = 3.4415290429024026e-05\n",
      "Epoch 59900: Training Loss = 5.440725999505958e-06\n",
      "Epoch 60000: Training Loss = 6.06529283686541e-06\n",
      "Lambda 1 = 0.9973323941230774, Lambda 2 = 0.0038135736249387264\n",
      "\n",
      "Epoch 60100: Training Loss = 0.00017376882897224277\n",
      "Epoch 60200: Training Loss = 5.377874003897887e-06\n",
      "Epoch 60300: Training Loss = 5.383591997087933e-05\n",
      "Epoch 60400: Training Loss = 1.6690115444362164e-05\n",
      "Epoch 60500: Training Loss = 5.416187832452124e-06\n",
      "Epoch 60600: Training Loss = 5.458236046251841e-06\n",
      "Epoch 60700: Training Loss = 8.782857184996828e-05\n",
      "Epoch 60800: Training Loss = 8.31597026262898e-06\n",
      "Epoch 60900: Training Loss = 2.3196214897325262e-05\n",
      "Epoch 61000: Training Loss = 0.00012247261474840343\n",
      "Lambda 1 = 0.9974308013916016, Lambda 2 = 0.0038037430495023727\n",
      "\n",
      "Epoch 61100: Training Loss = 5.2461518862401135e-06\n",
      "Epoch 61200: Training Loss = 8.138381417666096e-06\n",
      "Epoch 61300: Training Loss = 6.305311217147391e-06\n",
      "Epoch 61400: Training Loss = 6.0297606978565454e-06\n",
      "Epoch 61500: Training Loss = 5.210839844949078e-06\n",
      "Epoch 61600: Training Loss = 6.298057996900752e-06\n",
      "Epoch 61700: Training Loss = 1.9247952877776697e-05\n",
      "Epoch 61800: Training Loss = 1.011604217637796e-05\n",
      "Epoch 61900: Training Loss = 0.0001210886548506096\n",
      "Epoch 62000: Training Loss = 5.152925041329581e-06\n",
      "Lambda 1 = 0.9974517822265625, Lambda 2 = 0.0037952731363475323\n",
      "\n",
      "Epoch 62100: Training Loss = 1.0121741070179269e-05\n",
      "Epoch 62200: Training Loss = 1.9824961782433093e-05\n",
      "Epoch 62300: Training Loss = 2.5685012587928213e-05\n",
      "Epoch 62400: Training Loss = 7.339491276070476e-06\n",
      "Epoch 62500: Training Loss = 1.780036291165743e-05\n",
      "Epoch 62600: Training Loss = 2.319575651199557e-05\n",
      "Epoch 62700: Training Loss = 0.00010649887553881854\n",
      "Epoch 62800: Training Loss = 6.180796481203288e-05\n",
      "Epoch 62900: Training Loss = 5.291711204336025e-06\n",
      "Epoch 63000: Training Loss = 5.0685002861428075e-06\n",
      "Lambda 1 = 0.9974707961082458, Lambda 2 = 0.0037865962367504835\n",
      "\n",
      "Epoch 63100: Training Loss = 1.2211387911520433e-05\n",
      "Epoch 63200: Training Loss = 7.335409463848919e-05\n",
      "Epoch 63300: Training Loss = 2.8821377782151103e-05\n",
      "Epoch 63400: Training Loss = 6.2868173699826e-05\n",
      "Epoch 63500: Training Loss = 1.1509557225508615e-05\n",
      "Epoch 63600: Training Loss = 1.2162392522441223e-05\n",
      "Epoch 63700: Training Loss = 5.099045665701851e-06\n",
      "Epoch 63800: Training Loss = 5.806929402751848e-05\n",
      "Epoch 63900: Training Loss = 1.2682420674536843e-05\n",
      "Epoch 64000: Training Loss = 0.00014312985877040774\n",
      "Lambda 1 = 0.9976726770401001, Lambda 2 = 0.0037775214295834303\n",
      "\n",
      "Epoch 64100: Training Loss = 1.1583360901568085e-05\n",
      "Epoch 64200: Training Loss = 1.2475633411668241e-05\n",
      "Epoch 64300: Training Loss = 4.944474312651437e-06\n",
      "Epoch 64400: Training Loss = 6.124364517745562e-06\n",
      "Epoch 64500: Training Loss = 6.115732958278386e-06\n",
      "Epoch 64600: Training Loss = 1.1592708688112907e-05\n",
      "Epoch 64700: Training Loss = 5.1409042498562485e-06\n",
      "Epoch 64800: Training Loss = 8.66902519192081e-06\n",
      "Epoch 64900: Training Loss = 5.196761776460335e-06\n",
      "Epoch 65000: Training Loss = 4.866549716098234e-06\n",
      "Lambda 1 = 0.9974890351295471, Lambda 2 = 0.00376982893794775\n",
      "\n",
      "Epoch 65100: Training Loss = 4.768821781908628e-06\n",
      "Epoch 65200: Training Loss = 5.30828356204438e-06\n",
      "Epoch 65300: Training Loss = 1.5159982467594091e-05\n",
      "Epoch 65400: Training Loss = 1.780043385224417e-05\n",
      "Epoch 65500: Training Loss = 0.00013507503899745643\n",
      "Epoch 65600: Training Loss = 4.7952889872249216e-05\n",
      "Epoch 65700: Training Loss = 6.404778105206788e-06\n",
      "Epoch 65800: Training Loss = 6.498910806840286e-06\n",
      "Epoch 65900: Training Loss = 5.139841960044578e-06\n",
      "Epoch 66000: Training Loss = 5.66287781111896e-06\n",
      "Lambda 1 = 0.9975137710571289, Lambda 2 = 0.003761782543733716\n",
      "\n",
      "Epoch 66100: Training Loss = 1.864590376499109e-05\n",
      "Epoch 66200: Training Loss = 4.684915438701864e-06\n",
      "Epoch 66300: Training Loss = 4.713293037639232e-06\n",
      "Epoch 66400: Training Loss = 4.784152679349063e-06\n",
      "Epoch 66500: Training Loss = 4.756545422424097e-06\n",
      "Epoch 66600: Training Loss = 4.9912814574781805e-06\n",
      "Epoch 66700: Training Loss = 5.026146936870646e-06\n",
      "Epoch 66800: Training Loss = 4.6838731577736326e-06\n",
      "Epoch 66900: Training Loss = 5.230998795013875e-06\n",
      "Epoch 67000: Training Loss = 4.097589771845378e-05\n",
      "Lambda 1 = 0.9976847171783447, Lambda 2 = 0.003753635101020336\n",
      "\n",
      "Epoch 67100: Training Loss = 1.1322808859404176e-05\n",
      "Epoch 67200: Training Loss = 9.27475412026979e-05\n",
      "Epoch 67300: Training Loss = 6.388571819115896e-06\n",
      "Epoch 67400: Training Loss = 1.8643597286427394e-05\n",
      "Epoch 67500: Training Loss = 6.424973253160715e-05\n",
      "Epoch 67600: Training Loss = 1.1809146599262021e-05\n",
      "Epoch 67700: Training Loss = 4.685581643570913e-06\n",
      "Epoch 67800: Training Loss = 4.710208304459229e-05\n",
      "Epoch 67900: Training Loss = 1.1619333236012608e-05\n",
      "Epoch 68000: Training Loss = 1.9376446289243177e-05\n",
      "Lambda 1 = 0.9976463317871094, Lambda 2 = 0.0037461714819073677\n",
      "\n",
      "Epoch 68100: Training Loss = 2.031477379205171e-05\n",
      "Epoch 68200: Training Loss = 5.3245280469127465e-06\n",
      "Epoch 68300: Training Loss = 4.521302798821125e-06\n",
      "Epoch 68400: Training Loss = 9.568107998347841e-06\n",
      "Epoch 68500: Training Loss = 7.455922605004162e-05\n",
      "Epoch 68600: Training Loss = 0.00010486889368621632\n",
      "Epoch 68700: Training Loss = 1.3754646715824492e-05\n",
      "Epoch 68800: Training Loss = 4.614085810317192e-06\n",
      "Epoch 68900: Training Loss = 1.698489359114319e-05\n",
      "Epoch 69000: Training Loss = 5.0555026973597705e-06\n",
      "Lambda 1 = 0.9976528882980347, Lambda 2 = 0.0037387479096651077\n",
      "\n",
      "Epoch 69100: Training Loss = 5.019285708840471e-06\n",
      "Epoch 69200: Training Loss = 1.0904607734119054e-05\n",
      "Epoch 69300: Training Loss = 1.532600617792923e-05\n",
      "Epoch 69400: Training Loss = 2.2081294446252286e-05\n",
      "Epoch 69500: Training Loss = 1.1019179510185495e-05\n",
      "Epoch 69600: Training Loss = 7.120585905795451e-06\n",
      "Epoch 69700: Training Loss = 9.560419130139053e-05\n",
      "Epoch 69800: Training Loss = 1.9790786609519273e-05\n",
      "Epoch 69900: Training Loss = 4.399336376081919e-06\n",
      "Epoch 70000: Training Loss = 4.4532589527079836e-06\n",
      "Lambda 1 = 0.9976943731307983, Lambda 2 = 0.0037314651999622583\n",
      "\n",
      "Epoch 70100: Training Loss = 8.088660251814872e-06\n",
      "Epoch 70200: Training Loss = 5.971832251816522e-06\n",
      "Epoch 70300: Training Loss = 5.333693934517214e-06\n",
      "Epoch 70400: Training Loss = 1.7714835848892108e-05\n",
      "Epoch 70500: Training Loss = 6.49830762995407e-05\n",
      "Epoch 70600: Training Loss = 9.599465556675568e-06\n",
      "Epoch 70700: Training Loss = 4.583451300277375e-06\n",
      "Epoch 70800: Training Loss = 4.750419520860305e-06\n",
      "Epoch 70900: Training Loss = 0.00026982405688613653\n",
      "Epoch 71000: Training Loss = 9.856189535639714e-06\n",
      "Lambda 1 = 0.9977277517318726, Lambda 2 = 0.003724360140040517\n",
      "\n",
      "Epoch 71100: Training Loss = 0.0001924719545058906\n",
      "Epoch 71200: Training Loss = 4.3180275497434195e-06\n",
      "Epoch 71300: Training Loss = 9.339060852653347e-06\n",
      "Epoch 71400: Training Loss = 4.245509444444906e-06\n",
      "Epoch 71500: Training Loss = 1.5684705431340262e-05\n",
      "Epoch 71600: Training Loss = 4.693276423495263e-05\n",
      "Epoch 71700: Training Loss = 4.279171662346926e-06\n",
      "Epoch 71800: Training Loss = 5.35040999238845e-06\n",
      "Epoch 71900: Training Loss = 5.972628059680574e-06\n",
      "Epoch 72000: Training Loss = 2.1230411221040413e-05\n",
      "Lambda 1 = 0.9978037476539612, Lambda 2 = 0.0037173235323280096\n",
      "\n",
      "Epoch 72100: Training Loss = 1.0284627933287993e-05\n",
      "Epoch 72200: Training Loss = 1.2598317880474497e-05\n",
      "Epoch 72300: Training Loss = 9.11141250981018e-06\n",
      "Epoch 72400: Training Loss = 5.276478987070732e-06\n",
      "Epoch 72500: Training Loss = 1.1786213690356817e-05\n",
      "Epoch 72600: Training Loss = 1.7219617802766152e-05\n",
      "Epoch 72700: Training Loss = 4.778958100359887e-06\n",
      "Epoch 72800: Training Loss = 0.00014072749763727188\n",
      "Epoch 72900: Training Loss = 1.0057821782538667e-05\n",
      "Epoch 73000: Training Loss = 4.317906132200733e-06\n",
      "Lambda 1 = 0.9978023171424866, Lambda 2 = 0.0037106964737176895\n",
      "\n",
      "Epoch 73100: Training Loss = 1.4467742403212469e-05\n",
      "Epoch 73200: Training Loss = 4.9542040869710036e-06\n",
      "Epoch 73300: Training Loss = 4.183319106232375e-06\n",
      "Epoch 73400: Training Loss = 8.556467219023034e-05\n",
      "Epoch 73500: Training Loss = 1.3709957784158178e-05\n",
      "Epoch 73600: Training Loss = 4.318339051678777e-06\n",
      "Epoch 73700: Training Loss = 4.060520041093696e-06\n",
      "Epoch 73800: Training Loss = 4.780576273333281e-05\n",
      "Epoch 73900: Training Loss = 8.280593647214118e-06\n",
      "Epoch 74000: Training Loss = 1.3874286196369212e-05\n",
      "Lambda 1 = 0.9977871775627136, Lambda 2 = 0.0037040794268250465\n",
      "\n",
      "Epoch 74100: Training Loss = 3.4994533052667975e-05\n",
      "Epoch 74200: Training Loss = 2.409650187473744e-05\n",
      "Epoch 74300: Training Loss = 1.4718576494487934e-05\n",
      "Epoch 74400: Training Loss = 1.6458147001685575e-05\n",
      "Epoch 74500: Training Loss = 8.653681106807198e-06\n",
      "Epoch 74600: Training Loss = 2.251731712021865e-05\n",
      "Epoch 74700: Training Loss = 6.48989444016479e-05\n",
      "Epoch 74800: Training Loss = 5.616209818981588e-06\n",
      "Epoch 74900: Training Loss = 4.1707480704644695e-06\n",
      "Epoch 75000: Training Loss = 4.26650285589858e-06\n",
      "Lambda 1 = 0.9978527426719666, Lambda 2 = 0.0036976595874875784\n",
      "\n",
      "Epoch 75100: Training Loss = 6.068128641345538e-06\n",
      "Epoch 75200: Training Loss = 6.181865956023103e-06\n",
      "Epoch 75300: Training Loss = 1.6915950254769996e-05\n",
      "Epoch 75400: Training Loss = 5.8284890656068455e-06\n",
      "Epoch 75500: Training Loss = 1.0352247954870109e-05\n",
      "Epoch 75600: Training Loss = 2.2358361093210988e-05\n",
      "Epoch 75700: Training Loss = 6.938452315807808e-06\n",
      "Epoch 75800: Training Loss = 4.221783456159756e-06\n",
      "Epoch 75900: Training Loss = 4.799685939360643e-06\n",
      "Epoch 76000: Training Loss = 5.771592441305984e-06\n",
      "Lambda 1 = 0.997859537601471, Lambda 2 = 0.00369131937623024\n",
      "\n",
      "Epoch 76100: Training Loss = 5.840373887622263e-06\n",
      "Epoch 76200: Training Loss = 3.927234502043575e-06\n",
      "Epoch 76300: Training Loss = 4.393704784888541e-06\n",
      "Epoch 76400: Training Loss = 4.694943345384672e-05\n",
      "Epoch 76500: Training Loss = 4.801046543434495e-06\n",
      "Epoch 76600: Training Loss = 7.909065971034579e-06\n",
      "Epoch 76700: Training Loss = 3.8565078284591436e-05\n",
      "Epoch 76800: Training Loss = 4.023358087579254e-06\n",
      "Epoch 76900: Training Loss = 5.433957994682714e-06\n",
      "Epoch 77000: Training Loss = 4.06725257562357e-06\n",
      "Lambda 1 = 0.9978981614112854, Lambda 2 = 0.0036852112971246243\n",
      "\n",
      "Epoch 77100: Training Loss = 4.460360287339427e-06\n",
      "Epoch 77200: Training Loss = 7.258624918904388e-06\n",
      "Epoch 77300: Training Loss = 3.9725739043205976e-05\n",
      "Epoch 77400: Training Loss = 3.775852519538603e-06\n",
      "Epoch 77500: Training Loss = 4.068953785463236e-06\n",
      "Epoch 77600: Training Loss = 0.00016097631305456161\n",
      "Epoch 77700: Training Loss = 1.4300796465249732e-05\n",
      "Epoch 77800: Training Loss = 5.3475346248887945e-06\n",
      "Epoch 77900: Training Loss = 9.795883670449257e-06\n",
      "Epoch 78000: Training Loss = 4.307895324018318e-06\n",
      "Lambda 1 = 0.9979472756385803, Lambda 2 = 0.003679245011880994\n",
      "\n",
      "Epoch 78100: Training Loss = 4.7483868002018426e-06\n",
      "Epoch 78200: Training Loss = 1.1793914381996728e-05\n",
      "Epoch 78300: Training Loss = 8.551101200282574e-05\n",
      "Epoch 78400: Training Loss = 3.981968620792031e-05\n",
      "Epoch 78500: Training Loss = 7.227472451631911e-06\n",
      "Epoch 78600: Training Loss = 4.1635985326138325e-06\n",
      "Epoch 78700: Training Loss = 5.221006176725496e-06\n",
      "Epoch 78800: Training Loss = 3.8504622352775186e-05\n",
      "Epoch 78900: Training Loss = 9.01147177501116e-06\n",
      "Epoch 79000: Training Loss = 3.775337063416373e-06\n",
      "Lambda 1 = 0.9979972839355469, Lambda 2 = 0.0036732498556375504\n",
      "\n",
      "Epoch 79100: Training Loss = 3.6558358260663226e-06\n",
      "Epoch 79200: Training Loss = 3.375890082679689e-05\n",
      "Epoch 79300: Training Loss = 3.6013034332427196e-06\n",
      "Epoch 79400: Training Loss = 5.189185230847215e-06\n",
      "Epoch 79500: Training Loss = 3.7737490856670775e-06\n",
      "Epoch 79600: Training Loss = 4.7190760597004555e-06\n",
      "Epoch 79700: Training Loss = 5.160069122212008e-06\n",
      "Epoch 79800: Training Loss = 4.230103968438925e-06\n",
      "Epoch 79900: Training Loss = 3.6194221593177645e-06\n",
      "Epoch 80000: Training Loss = 1.4667846699012443e-05\n",
      "Lambda 1 = 0.9979277849197388, Lambda 2 = 0.0036676404997706413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pinn.fit(epochs = 80000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2e42f1c9e64f90960862245e610091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the results\n",
    "%matplotlib widget\n",
    "device = torch.device('cpu')\n",
    "\n",
    "plot_results_discrete_identification(x, t, x0.to(device).detach().numpy(), x1.to(device).detach().numpy(), u_exact,\n",
    "                                     u0.to(device).detach().numpy(), u1.to(device).detach().numpy(),\n",
    "                                     idx_t0, idx_t1, lb, ub,\n",
    "                                     pinn.lmbda[0].to(device).detach().item(), torch.exp(pinn.lmbda[1]).to(device).detach().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
